{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import torch\n",
    "\n",
    "from wittgenstein.irep import IREP\n",
    "from wittgenstein.ripper import RIPPER\n",
    "from wittgenstein.interpret import interpret_model, score_fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0   True    14.23        1.71  2.43               15.6        127   \n",
       "1   True    13.20        1.78  2.14               11.2        100   \n",
       "2   True    13.16        2.36  2.67               18.6        101   \n",
       "3   True    14.37        1.95  2.50               16.8        113   \n",
       "4   True    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and transform target to binary classification \n",
    "\n",
    "inpath = ''\n",
    "df = pd.read_csv(inpath + \"wine.csv\")\n",
    "df['Class'] = df['Class'].map(lambda x: True if x==1 else False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test-split\n",
    "\n",
    "X, y = df.drop('Class', axis=1), df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wittgenstein classifier for (global surrogate) interpretation.\n",
    "\n",
    "interpreter = RIPPER(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Proline=>1227.0] V\n",
      "[Proline=1048.0-1227.0] V\n",
      "[Proline=880.0-1048.0]]\n"
     ]
    }
   ],
   "source": [
    "# Fit and interpret SVM.\n",
    "# interpret_model fits the interpreter to the model.\n",
    "\n",
    "svc = SVC(kernel='rbf', random_state=42)\n",
    "svc.fit(X_train, y_train);\n",
    "\n",
    "interpret_model(model=svc, X=X_train, interpreter=interpreter).out_pretty()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score how well the interpreter's predictions match the underlying model's\n",
    "\n",
    "score_fidelity(\n",
    "    X_test,\n",
    "    interpreter,\n",
    "    model=svc,\n",
    "    score_function=[precision_score, recall_score, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Proline=>1227.0] V\n",
      "[Proline=1048.0-1227.0] V\n",
      "[Proline=880.0-1048.0 ^ Ash=2.0-2.19] V\n",
      "[Proline=880.0-1048.0 ^ Malicacid=1.76-1.9] V\n",
      "[Colorintensity=5.75-6.78]]\n"
     ]
    }
   ],
   "source": [
    "# Interpret Random Forest\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "interpret_model(model=rf, X=X_train, interpreter=interpreter).out_pretty()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.7333333333333333, 0.846153846153846]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_fidelity(\n",
    "    X_test,\n",
    "    interpreter,\n",
    "    model=rf,\n",
    "    score_function=[precision_score, recall_score, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - 1s 2ms/step - loss: 4.2813 - accuracy: 0.7293\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 1.8663 - accuracy: 0.7744\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 3.7320 - accuracy: 0.7143\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 1.7408 - accuracy: 0.8271\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 1.4441 - accuracy: 0.8271\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 1.7489 - accuracy: 0.8045\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 2.2349 - accuracy: 0.8195\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 1.3922 - accuracy: 0.8421\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.1142 - accuracy: 0.8872\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 1.4097 - accuracy: 0.8647\n",
      "[[Proline=880.0-1048.0] V\n",
      "[Proline=>1227.0] V\n",
      "[Proline=1048.0-1227.0] V\n",
      "[Alcalinityofash=16.8-17.72]]\n"
     ]
    }
   ],
   "source": [
    "# Interpret Tensorflow/Keras model\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(60, input_dim=13, activation='relu'))\n",
    "mlp.add(Dense(30, activation='relu'))\n",
    "mlp.add(Dense(1, activation='sigmoid'))\n",
    "mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "mlp.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=1,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "interpret_model(model=mlp, X=X_train, interpreter=interpreter).out_pretty()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.7916666666666666, 0.8837209302325582]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_fidelity(\n",
    "    X_test,\n",
    "    interpreter,\n",
    "    model=mlp,\n",
    "    score_function=[precision_score, recall_score, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf preds: [ True False False  True False]\n",
      "\n",
      "interpretion:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[<Rule [Proline=880.0-1048.0]>],\n",
       " [],\n",
       " [],\n",
       " [<Rule [Alcalinityofash=16.8-17.72]>],\n",
       " []]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since interpret_model fits a wittgenstein classifier,\n",
    "# we can do with it all the normal things a wittgenstein classifier can do.\n",
    "# Here, we'll use it to approximate what in the data the tensorflow model\n",
    "# is picking up on when it makes specific positive predictions.\n",
    "\n",
    "preds = (mlp.predict(X_test.tail()) > .5).flatten()\n",
    "_, interpretation = interpreter.predict(X_test.tail(), give_reasons=True)\n",
    "\n",
    "print(f'tf preds: {preds}\\n')\n",
    "print('interpretion:')\n",
    "interpretation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wittgenstein",
   "language": "python",
   "name": "wittgenstein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
